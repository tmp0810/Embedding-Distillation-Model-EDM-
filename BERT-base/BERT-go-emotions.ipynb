{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"  pip install transformers datasets evaluate","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-25T04:02:34.802126Z","iopub.execute_input":"2025-03-25T04:02:34.802516Z","iopub.status.idle":"2025-03-25T04:02:39.480284Z","shell.execute_reply.started":"2025-03-25T04:02:34.802471Z","shell.execute_reply":"2025-03-25T04:02:39.479426Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.47.0)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nCollecting evaluate\n  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.17.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.29.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.21.0)\nRequirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->transformers) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.3\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!mkdir -p ~/.kaggle\n!echo '{\"wandb_api_key\": \"5af3f6dab5b4be7bfb38dadf6554a3d40a09ada9\"}' > ~/.kaggle/secrets.json\n!chmod 600 ~/.kaggle/secrets.json\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T04:03:53.098254Z","iopub.status.idle":"2025-03-25T04:03:53.098574Z","shell.execute_reply":"2025-03-25T04:03:53.098437Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport wandb\n\n# Load API Key tá»« Kaggle Secrets\nwith open(\"/root/.kaggle/secrets.json\", \"r\") as f:\n    secrets = json.load(f)\n    wandb_api_key = secrets[\"wandb_api_key\"]\n\n# ÄÄƒng nháº­p vÃ o W&B\nwandb.login(key=wandb_api_key)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T04:02:39.823078Z","iopub.execute_input":"2025-03-25T04:02:39.823305Z","iopub.status.idle":"2025-03-25T04:02:48.020062Z","shell.execute_reply.started":"2025-03-25T04:02:39.823285Z","shell.execute_reply":"2025-03-25T04:02:48.019216Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtruongminhphuc08102005\u001b[0m (\u001b[33mtruongminhphuc08102005-hanoi-university-of-science-and-t\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer, \n    AutoModelForSequenceClassification, \n    Trainer, \n    TrainingArguments\n)\nfrom sklearn.metrics import precision_recall_fscore_support\n\n# 1. Load the GoEmotions simplified dataset\ndataset = load_dataset(\"google-research-datasets/go_emotions\", \"simplified\")\nnum_labels = dataset[\"train\"].features[\"labels\"].feature.num_classes\n\n# 2. Load the BERT tokenizer and model\nmodel_name = \"bert-base-uncased\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=num_labels,\n    problem_type=\"multi_label_classification\"\n)\n\n# 3. Preprocessing function: tokenize text and convert label lists to multi-hot vectors as floats\ndef preprocess_function(examples):\n    tokenized_inputs = tokenizer(\n        examples[\"text\"], \n        truncation=True, \n        padding=\"max_length\", \n        max_length=128\n    )\n    multi_hot_labels = []\n    for label_list in examples[\"labels\"]:\n        vector = [0] * num_labels\n        for label in label_list:\n            vector[label] = 1\n        # Convert to floats\n        multi_hot_labels.append([float(x) for x in vector])\n    tokenized_inputs[\"labels\"] = multi_hot_labels\n    return tokenized_inputs\n\n# Apply preprocessing and remove original columns\ntokenized_datasets = dataset.map(\n    preprocess_function, \n    batched=True, \n    remove_columns=[\"text\", \"labels\", \"id\"]\n)\n\n# 4. Set the dataset format to torch for our columns, specifying that labels are floats.\ntokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n\n# 5. Create a custom data collator to ensure labels are float tensors\n'''def custom_collator(features):\n    batch = {}\n    for key in features[0].keys():\n        if key == \"labels\":\n            # Convert each label list to a tensor and stack\n            batch[\"labels\"] = torch.stack([torch.tensor(f[key], dtype=torch.float) for f in features])\n        else:\n            batch[key] = torch.stack([torch.tensor(f[key]) for f in features])\n    return batch'''\n# 5. Create a custom data collator that uses clone().detach() to avoid warnings\ndef custom_collator(features):\n    batch = {}\n    for key in features[0].keys():\n        # If the feature is already a tensor, clone and detach it\n        if key == \"labels\":\n            batch[\"labels\"] = torch.stack([f[key].clone().detach().float() for f in features])\n        else:\n            batch[key] = torch.stack([f[key].clone().detach() for f in features])\n    return batch\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T04:31:44.158572Z","iopub.execute_input":"2025-03-25T04:31:44.158912Z","iopub.status.idle":"2025-03-25T04:31:45.768960Z","shell.execute_reply.started":"2025-03-25T04:31:44.158883Z","shell.execute_reply":"2025-03-25T04:31:45.768308Z"}},"outputs":[{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"# 6. Define compute_metrics for evaluation\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    # Convert logits to probabilities\n    probs = torch.sigmoid(torch.tensor(logits)).numpy()\n    # Apply threshold to get binary predictions\n    preds = (probs > 0.5).astype(int)\n    # Convert labels to int for scikit-learn metrics\n    labels_int = labels.astype(int)\n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels_int, preds, average=\"micro\", zero_division=0\n    )\n    # Exact match accuracy: all labels for a sample must be correct\n    exact_match_acc = (preds == labels_int).all(axis=1).mean()\n    return {\"accuracy\": exact_match_acc, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n\n# 7. Define training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    evaluation_strategy=\"epoch\",  # Evaluate at the end of each epoch\n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=3,\n    weight_decay=0.01,\n)\n\n# 8. Initialize the Trainer with the custom data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],\n    eval_dataset=tokenized_datasets[\"validation\"],\n    tokenizer=tokenizer,\n    data_collator=custom_collator,\n    compute_metrics=compute_metrics,\n)\n\n# 9. Fine-tune the model\ntrainer.train()\n\n# 10. Evaluate on the test set\ntest_results = trainer.evaluate(tokenized_datasets[\"test\"])\nprint(\"Test set evaluation results:\")\nprint(test_results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T04:31:54.583239Z","iopub.execute_input":"2025-03-25T04:31:54.583591Z","execution_failed":"2025-03-25T04:32:14.011Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n<ipython-input-30-704fb4de50af>:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.10/dist-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='43' max='4071' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [  43/4071 00:18 < 29:52, 2.25 it/s, Epoch 0.03/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}],"execution_count":null}]}