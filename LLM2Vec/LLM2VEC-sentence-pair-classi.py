# -*- coding: utf-8 -*-
"""Mistral_SentencePair_Broad-Coverageipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TSUtCpQvK3vpGlFWo0I7qziWnvJxlxWT
"""

!pip install -q transformers accelerate trl bitsandbytes datasets evaluate peft scikit-learn
!pip install llm2vec

from llm2vec import LLM2Vec
import torch
from transformers import AutoTokenizer, AutoModel, AutoConfig
from peft import PeftModel, LoraConfig, get_peft_model
from datasets import load_dataset
from torch.utils.data import DataLoader
import torch.nn as nn
from sklearn.metrics import accuracy_score, precision_score, recall_score
from huggingface_hub import login

login(token="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")

# Loading base Mistral model, along with custom code that enables bidirectional connections in decoder-only LLMs. MNTP LoRA weights are merged into the base model.
tokenizer = AutoTokenizer.from_pretrained(
    "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp"
)
config = AutoConfig.from_pretrained(
    "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp", trust_remote_code=True
)
model = AutoModel.from_pretrained(
    "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp",
    trust_remote_code=True,
    config=config,
    torch_dtype=torch.bfloat16,
    device_map="cuda" if torch.cuda.is_available() else "cpu",
)
model = PeftModel.from_pretrained(
    model,
    "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp",
)
model = model.merge_and_unload()  # This can take several minutes on cpu

# Loading unsupervised SimCSE model. This loads the trained LoRA weights on top of MNTP model.
model = PeftModel.from_pretrained(
    model, "McGill-NLP/LLM2Vec-Mistral-7B-Instruct-v2-mntp-unsup-simcse"
)


# Merge SimCSE LoRA vào base model
model = model.merge_and_unload()

# Thêm LoRA adapter mới cho task classification
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.1,
    bias="none",
)
model_with_lora = get_peft_model(model, lora_config)

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from datasets import load_dataset
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score

class NLIClassifier(nn.Module):
    def __init__(self, base_model, num_classes=3):
        super().__init__()
        self.base_model = base_model
        hidden_size = base_model.config.hidden_size  # Embedding size
        self.classifier = nn.Linear(2 * hidden_size, num_classes)  # Concatenate 2 embeddings, so double the size

    def forward(self, premise_input_ids, premise_attention_mask, hypothesis_input_ids, hypothesis_attention_mask):
        # Encode premise
        premise_outputs = self.base_model(input_ids=premise_input_ids, attention_mask=premise_attention_mask)
        premise_hidden = premise_outputs.last_hidden_state
        premise_mask = premise_attention_mask.unsqueeze(-1).expand(premise_hidden.size())
        premise_sum = (premise_hidden * premise_mask).sum(dim=1)
        premise_count = premise_mask.sum(dim=1)
        premise_emb = premise_sum / premise_count  # Mean pooling

        # Encode hypothesis
        hypothesis_outputs = self.base_model(input_ids=hypothesis_input_ids, attention_mask=hypothesis_attention_mask)
        hypothesis_hidden = hypothesis_outputs.last_hidden_state
        hypothesis_mask = hypothesis_attention_mask.unsqueeze(-1).expand(hypothesis_hidden.size())
        hypothesis_sum = (hypothesis_hidden * hypothesis_mask).sum(dim=1)
        hypothesis_count = hypothesis_mask.sum(dim=1)
        hypothesis_emb = hypothesis_sum / hypothesis_count  # Mean pooling

        # Concatenate the two embeddings and cast to float32
        combined_emb = torch.cat([premise_emb, hypothesis_emb], dim=1).to(torch.float32)
        logits = self.classifier(combined_emb)
        return logits

classifier = NLIClassifier(model_with_lora)
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
classifier.to(device)

# dataset
dataset = load_dataset("nyu-mll/multi_nli")
dataset["train"] = dataset["train"]
dataset["validation_matched"] = dataset["validation_matched"]
dataset["validation_mismatched"] = dataset["validation_mismatched"]
dataset = dataset.filter(lambda x: x["label"] != -1)

#tokenize
def tokenize_function(examples):
    premise_tokenized = tokenizer(examples["premise"], padding="max_length", truncation=True, max_length=128)
    hypothesis_tokenized = tokenizer(examples["hypothesis"], padding="max_length", truncation=True, max_length=128)
    return {
        "premise_input_ids": premise_tokenized["input_ids"],
        "premise_attention_mask": premise_tokenized["attention_mask"],
        "hypothesis_input_ids": hypothesis_tokenized["input_ids"],
        "hypothesis_attention_mask": hypothesis_tokenized["attention_mask"],
        "labels": examples["label"],
    }

tokenized_dataset = dataset.map(tokenize_function, batched=True)
tokenized_dataset.set_format("torch", columns=["premise_input_ids", "premise_attention_mask", "hypothesis_input_ids", "hypothesis_attention_mask", "labels"])

# DataLoader
batch_size = 16
train_loader = DataLoader(tokenized_dataset["train"], batch_size=batch_size, shuffle=True)
val_matched_loader = DataLoader(tokenized_dataset["validation_matched"], batch_size=batch_size)
val_mismatched_loader = DataLoader(tokenized_dataset["validation_mismatched"], batch_size=batch_size)

# train
optimizer = torch.optim.AdamW(classifier.parameters(), lr=1e-4)
num_epochs = 3

for epoch in range(num_epochs):
    classifier.train()
    train_loop = tqdm(train_loader, desc=f"Training Epoch {epoch + 1}/{num_epochs}", leave=True)
    for batch in train_loop:
        premise_input_ids = batch["premise_input_ids"].to(device)
        premise_attention_mask = batch["premise_attention_mask"].to(device)
        hypothesis_input_ids = batch["hypothesis_input_ids"].to(device)
        hypothesis_attention_mask = batch["hypothesis_attention_mask"].to(device)
        labels = batch["labels"].to(device)

        outputs = classifier(premise_input_ids, premise_attention_mask, hypothesis_input_ids, hypothesis_attention_mask)
        loss = nn.CrossEntropyLoss()(outputs, labels)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loop.set_postfix(loss=loss.item())
    print(f"Epoch {epoch + 1}/{num_epochs} completed")


def evaluate(loader, split_name):
    classifier.eval()
    all_preds = []
    all_labels = []
    eval_loop = tqdm(loader, desc=f"Evaluating {split_name}", leave=True)
    with torch.no_grad():
        for batch in eval_loop:
            premise_input_ids = batch["premise_input_ids"].to(device)
            premise_attention_mask = batch["premise_attention_mask"].to(device)
            hypothesis_input_ids = batch["hypothesis_input_ids"].to(device)
            hypothesis_attention_mask = batch["hypothesis_attention_mask"].to(device)
            labels = batch["labels"].to(device)

            outputs = classifier(premise_input_ids, premise_attention_mask, hypothesis_input_ids, hypothesis_attention_mask)
            preds = torch.argmax(outputs, dim=1)
            all_preds.extend(preds.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())

    accuracy = accuracy_score(all_labels, all_preds)
    precision = precision_score(all_labels, all_preds, average="macro")
    recall = recall_score(all_labels, all_preds, average="macro")
    print(f"{split_name} - Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}")
    return accuracy, precision, recall

# Đánh giá trên validation_matched và validation_mismatched
print("\n validation:")
evaluate(val_matched_loader, "Validation Matched")
evaluate(val_mismatched_loader, "Validation Mismatched")